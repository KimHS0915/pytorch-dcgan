{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan_face.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHv1O-fTvwsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYZAz3OtDTFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRXpXq8GYFZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bKmv7pGDiI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataroot = \"/content/gdrive/My Drive/Colab Notebooks/pytorch/face\"\n",
        "workers = 2\n",
        "batch_size = 128\n",
        "nc = 3\n",
        "nz = 100\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "num_epochs = 100\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "ngpu = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDHvnYSiEUkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dset.ImageFolder(root=dataroot,\n",
        "                           transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                                         transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                                                             (0.5, 0.5, 0.5))]))\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPQhMpXN38E1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2,\n",
        "                                         normalize=True).cpu(),(1,2,0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofesDtW-Zsd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQbRjcfjaStq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf*2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gHFOg6wdQEE",
        "colab_type": "code",
        "outputId": "97575cbe-3548-4c46-a0b2-03ba6e166d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netg = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "netG.apply(weights_init)\n",
        "\n",
        "print(netG)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lORhJMkelth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf*8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0XRDmdCj6da",
        "colab_type": "code",
        "outputId": "cacf9750-5599-4c55-f133-a67d678a7f71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "\n",
        "netD.apply(weights_init)\n",
        "\n",
        "print(netD)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4R8pnJXkgxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh5mkWGqk5cY",
        "colab_type": "code",
        "outputId": "9708866f-acd8-4989-87a7-4a8ea63d8233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Update D network\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, device=device)\n",
        "\n",
        "        output = netD(real_cpu).view(-1)\n",
        "\n",
        "        errD_real = criterion(output, label)\n",
        "\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "\n",
        "        fake=netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "\n",
        "        errD_fake = criterion(output, label)\n",
        "\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "\n",
        "        errD = errD_real + errD_fake\n",
        "\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Update G network\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)\n",
        "\n",
        "        output = netD(fake).view(-1)\n",
        "\n",
        "        errG = criterion(output, label)\n",
        "\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        \n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "        iters += 1 "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training Loop...\n",
            "[0/100][0/29]\tLoss_D: 1.7477\tLoss_G: 2.2831\tD(x): 0.3276\tD(G(z)): 0.2659 / 0.1434\n",
            "[1/100][0/29]\tLoss_D: 0.0357\tLoss_G: 12.0992\tD(x): 0.9799\tD(G(z)): 0.0000 / 0.0000\n",
            "[2/100][0/29]\tLoss_D: 0.1075\tLoss_G: 6.4832\tD(x): 0.9379\tD(G(z)): 0.0053 / 0.0032\n",
            "[3/100][0/29]\tLoss_D: 2.1385\tLoss_G: 7.5098\tD(x): 0.3261\tD(G(z)): 0.0000 / 0.0011\n",
            "[4/100][0/29]\tLoss_D: 1.7660\tLoss_G: 16.3440\tD(x): 0.4727\tD(G(z)): 0.0000 / 0.0000\n",
            "[5/100][0/29]\tLoss_D: 1.0454\tLoss_G: 10.7872\tD(x): 0.9068\tD(G(z)): 0.5092 / 0.0000\n",
            "[6/100][0/29]\tLoss_D: 0.6639\tLoss_G: 9.4474\tD(x): 0.6914\tD(G(z)): 0.0016 / 0.0003\n",
            "[7/100][0/29]\tLoss_D: 0.4067\tLoss_G: 6.7447\tD(x): 0.7668\tD(G(z)): 0.0075 / 0.0017\n",
            "[8/100][0/29]\tLoss_D: 1.3276\tLoss_G: 14.5366\tD(x): 0.9620\tD(G(z)): 0.6077 / 0.0000\n",
            "[9/100][0/29]\tLoss_D: 0.3937\tLoss_G: 6.3770\tD(x): 0.9198\tD(G(z)): 0.2107 / 0.0044\n",
            "[10/100][0/29]\tLoss_D: 0.6598\tLoss_G: 9.5712\tD(x): 0.6828\tD(G(z)): 0.0022 / 0.0011\n",
            "[11/100][0/29]\tLoss_D: 1.2629\tLoss_G: 14.2658\tD(x): 0.9584\tD(G(z)): 0.6387 / 0.0000\n",
            "[12/100][0/29]\tLoss_D: 0.4780\tLoss_G: 3.7530\tD(x): 0.7833\tD(G(z)): 0.0341 / 0.0625\n",
            "[13/100][0/29]\tLoss_D: 1.4017\tLoss_G: 14.2445\tD(x): 0.8828\tD(G(z)): 0.6294 / 0.0000\n",
            "[14/100][0/29]\tLoss_D: 0.4275\tLoss_G: 4.7068\tD(x): 0.7851\tD(G(z)): 0.0540 / 0.0162\n",
            "[15/100][0/29]\tLoss_D: 0.1735\tLoss_G: 4.8178\tD(x): 0.9573\tD(G(z)): 0.0761 / 0.0258\n",
            "[16/100][0/29]\tLoss_D: 0.3182\tLoss_G: 6.2522\tD(x): 0.9211\tD(G(z)): 0.1781 / 0.0041\n",
            "[17/100][0/29]\tLoss_D: 0.6451\tLoss_G: 7.6078\tD(x): 0.9338\tD(G(z)): 0.3695 / 0.0019\n",
            "[18/100][0/29]\tLoss_D: 0.3546\tLoss_G: 3.5608\tD(x): 0.8336\tD(G(z)): 0.0890 / 0.0491\n",
            "[19/100][0/29]\tLoss_D: 0.1109\tLoss_G: 4.8668\tD(x): 0.9275\tD(G(z)): 0.0171 / 0.0132\n",
            "[20/100][0/29]\tLoss_D: 0.5249\tLoss_G: 7.4154\tD(x): 0.9486\tD(G(z)): 0.3395 / 0.0014\n",
            "[21/100][0/29]\tLoss_D: 0.2442\tLoss_G: 4.8218\tD(x): 0.9464\tD(G(z)): 0.1451 / 0.0152\n",
            "[22/100][0/29]\tLoss_D: 0.4527\tLoss_G: 6.8620\tD(x): 0.9520\tD(G(z)): 0.2939 / 0.0020\n",
            "[23/100][0/29]\tLoss_D: 0.3107\tLoss_G: 4.8310\tD(x): 0.8489\tD(G(z)): 0.0868 / 0.0146\n",
            "[24/100][0/29]\tLoss_D: 1.1544\tLoss_G: 8.0320\tD(x): 0.9519\tD(G(z)): 0.5707 / 0.0012\n",
            "[25/100][0/29]\tLoss_D: 0.2276\tLoss_G: 5.4019\tD(x): 0.9138\tD(G(z)): 0.1092 / 0.0070\n",
            "[26/100][0/29]\tLoss_D: 1.7785\tLoss_G: 6.0149\tD(x): 0.3026\tD(G(z)): 0.0006 / 0.0099\n",
            "[27/100][0/29]\tLoss_D: 0.2616\tLoss_G: 3.8053\tD(x): 0.8204\tD(G(z)): 0.0178 / 0.0500\n",
            "[28/100][0/29]\tLoss_D: 0.3540\tLoss_G: 5.5475\tD(x): 0.9604\tD(G(z)): 0.2408 / 0.0065\n",
            "[29/100][0/29]\tLoss_D: 0.2402\tLoss_G: 5.2751\tD(x): 0.8379\tD(G(z)): 0.0254 / 0.0096\n",
            "[30/100][0/29]\tLoss_D: 0.9553\tLoss_G: 1.9624\tD(x): 0.5555\tD(G(z)): 0.0679 / 0.2070\n",
            "[31/100][0/29]\tLoss_D: 0.4521\tLoss_G: 4.6956\tD(x): 0.8508\tD(G(z)): 0.1935 / 0.0155\n",
            "[32/100][0/29]\tLoss_D: 0.3891\tLoss_G: 3.3999\tD(x): 0.8164\tD(G(z)): 0.1119 / 0.0554\n",
            "[33/100][0/29]\tLoss_D: 0.8030\tLoss_G: 6.0963\tD(x): 0.9739\tD(G(z)): 0.4633 / 0.0056\n",
            "[34/100][0/29]\tLoss_D: 0.5003\tLoss_G: 6.3677\tD(x): 0.9105\tD(G(z)): 0.2988 / 0.0032\n",
            "[35/100][0/29]\tLoss_D: 2.4888\tLoss_G: 0.4172\tD(x): 0.1797\tD(G(z)): 0.0143 / 0.7279\n",
            "[36/100][0/29]\tLoss_D: 0.2601\tLoss_G: 3.4331\tD(x): 0.8933\tD(G(z)): 0.1142 / 0.0493\n",
            "[37/100][0/29]\tLoss_D: 0.5412\tLoss_G: 5.6673\tD(x): 0.9757\tD(G(z)): 0.3401 / 0.0069\n",
            "[38/100][0/29]\tLoss_D: 0.4296\tLoss_G: 5.3006\tD(x): 0.9294\tD(G(z)): 0.2653 / 0.0089\n",
            "[39/100][0/29]\tLoss_D: 1.4179\tLoss_G: 4.1109\tD(x): 0.3477\tD(G(z)): 0.0021 / 0.0420\n",
            "[40/100][0/29]\tLoss_D: 1.2299\tLoss_G: 9.7955\tD(x): 0.9798\tD(G(z)): 0.6341 / 0.0002\n",
            "[41/100][0/29]\tLoss_D: 1.8139\tLoss_G: 0.6819\tD(x): 0.2716\tD(G(z)): 0.0024 / 0.6144\n",
            "[42/100][0/29]\tLoss_D: 0.5273\tLoss_G: 5.4005\tD(x): 0.9530\tD(G(z)): 0.3383 / 0.0083\n",
            "[43/100][0/29]\tLoss_D: 1.2609\tLoss_G: 3.0226\tD(x): 0.4092\tD(G(z)): 0.0038 / 0.0813\n",
            "[44/100][0/29]\tLoss_D: 0.5219\tLoss_G: 3.7847\tD(x): 0.7614\tD(G(z)): 0.1554 / 0.0328\n",
            "[45/100][0/29]\tLoss_D: 0.6761\tLoss_G: 6.6099\tD(x): 0.9229\tD(G(z)): 0.4062 / 0.0025\n",
            "[46/100][0/29]\tLoss_D: 0.6341\tLoss_G: 6.4812\tD(x): 0.9825\tD(G(z)): 0.4116 / 0.0030\n",
            "[47/100][0/29]\tLoss_D: 0.6650\tLoss_G: 2.3429\tD(x): 0.6103\tD(G(z)): 0.0311 / 0.1369\n",
            "[48/100][0/29]\tLoss_D: 1.4286\tLoss_G: 7.6065\tD(x): 0.9963\tD(G(z)): 0.6925 / 0.0019\n",
            "[49/100][0/29]\tLoss_D: 0.7161\tLoss_G: 6.8245\tD(x): 0.9868\tD(G(z)): 0.4435 / 0.0023\n",
            "[50/100][0/29]\tLoss_D: 2.9555\tLoss_G: 0.9089\tD(x): 0.1057\tD(G(z)): 0.0007 / 0.4870\n",
            "[51/100][0/29]\tLoss_D: 0.6786\tLoss_G: 2.9077\tD(x): 0.5805\tD(G(z)): 0.0125 / 0.0789\n",
            "[52/100][0/29]\tLoss_D: 1.6901\tLoss_G: 10.9956\tD(x): 0.9871\tD(G(z)): 0.7438 / 0.0001\n",
            "[53/100][0/29]\tLoss_D: 0.6355\tLoss_G: 2.0984\tD(x): 0.6005\tD(G(z)): 0.0409 / 0.1702\n",
            "[54/100][0/29]\tLoss_D: 0.4929\tLoss_G: 5.1614\tD(x): 0.9233\tD(G(z)): 0.3082 / 0.0087\n",
            "[55/100][0/29]\tLoss_D: 0.9197\tLoss_G: 1.7178\tD(x): 0.4724\tD(G(z)): 0.0126 / 0.2501\n",
            "[56/100][0/29]\tLoss_D: 1.1721\tLoss_G: 1.8526\tD(x): 0.4130\tD(G(z)): 0.0083 / 0.2224\n",
            "[57/100][0/29]\tLoss_D: 0.4263\tLoss_G: 2.7872\tD(x): 0.7947\tD(G(z)): 0.1400 / 0.0886\n",
            "[58/100][0/29]\tLoss_D: 0.3595\tLoss_G: 3.3297\tD(x): 0.8404\tD(G(z)): 0.1478 / 0.0587\n",
            "[59/100][0/29]\tLoss_D: 0.2732\tLoss_G: 3.7803\tD(x): 0.8683\tD(G(z)): 0.1092 / 0.0333\n",
            "[60/100][0/29]\tLoss_D: 0.3900\tLoss_G: 4.3940\tD(x): 0.7470\tD(G(z)): 0.0405 / 0.0244\n",
            "[61/100][0/29]\tLoss_D: 0.2140\tLoss_G: 4.1442\tD(x): 0.8903\tD(G(z)): 0.0858 / 0.0237\n",
            "[62/100][0/29]\tLoss_D: 0.4903\tLoss_G: 5.1327\tD(x): 0.9091\tD(G(z)): 0.2918 / 0.0095\n",
            "[63/100][0/29]\tLoss_D: 0.9693\tLoss_G: 7.5191\tD(x): 0.9866\tD(G(z)): 0.5573 / 0.0012\n",
            "[64/100][0/29]\tLoss_D: 0.4601\tLoss_G: 3.1468\tD(x): 0.6885\tD(G(z)): 0.0367 / 0.0697\n",
            "[65/100][0/29]\tLoss_D: 0.3371\tLoss_G: 2.5208\tD(x): 0.8068\tD(G(z)): 0.0952 / 0.1123\n",
            "[66/100][0/29]\tLoss_D: 0.4585\tLoss_G: 3.5038\tD(x): 0.6904\tD(G(z)): 0.0204 / 0.0617\n",
            "[67/100][0/29]\tLoss_D: 0.4396\tLoss_G: 2.9680\tD(x): 0.7131\tD(G(z)): 0.0470 / 0.0873\n",
            "[68/100][0/29]\tLoss_D: 0.3105\tLoss_G: 3.5638\tD(x): 0.8976\tD(G(z)): 0.1603 / 0.0440\n",
            "[69/100][0/29]\tLoss_D: 0.3764\tLoss_G: 2.1815\tD(x): 0.7559\tD(G(z)): 0.0559 / 0.1458\n",
            "[70/100][0/29]\tLoss_D: 0.6470\tLoss_G: 2.3040\tD(x): 0.6475\tD(G(z)): 0.1007 / 0.1530\n",
            "[71/100][0/29]\tLoss_D: 0.5580\tLoss_G: 5.4811\tD(x): 0.9661\tD(G(z)): 0.3709 / 0.0063\n",
            "[72/100][0/29]\tLoss_D: 0.2847\tLoss_G: 3.1939\tD(x): 0.8650\tD(G(z)): 0.1178 / 0.0547\n",
            "[73/100][0/29]\tLoss_D: 0.3029\tLoss_G: 3.6933\tD(x): 0.9158\tD(G(z)): 0.1772 / 0.0346\n",
            "[74/100][0/29]\tLoss_D: 0.4500\tLoss_G: 2.8315\tD(x): 0.7852\tD(G(z)): 0.1464 / 0.0940\n",
            "[75/100][0/29]\tLoss_D: 0.2915\tLoss_G: 3.5827\tD(x): 0.8953\tD(G(z)): 0.1530 / 0.0398\n",
            "[76/100][0/29]\tLoss_D: 0.2603\tLoss_G: 2.8636\tD(x): 0.8577\tD(G(z)): 0.0884 / 0.0767\n",
            "[77/100][0/29]\tLoss_D: 0.6325\tLoss_G: 1.4435\tD(x): 0.6655\tD(G(z)): 0.0732 / 0.3246\n",
            "[78/100][0/29]\tLoss_D: 0.4825\tLoss_G: 4.7802\tD(x): 0.9636\tD(G(z)): 0.3156 / 0.0145\n",
            "[79/100][0/29]\tLoss_D: 0.3570\tLoss_G: 4.0675\tD(x): 0.9243\tD(G(z)): 0.2181 / 0.0246\n",
            "[80/100][0/29]\tLoss_D: 0.3685\tLoss_G: 2.9063\tD(x): 0.7743\tD(G(z)): 0.0732 / 0.0805\n",
            "[81/100][0/29]\tLoss_D: 0.2495\tLoss_G: 3.3585\tD(x): 0.9009\tD(G(z)): 0.1244 / 0.0499\n",
            "[82/100][0/29]\tLoss_D: 0.7722\tLoss_G: 1.6648\tD(x): 0.6095\tD(G(z)): 0.1169 / 0.2633\n",
            "[83/100][0/29]\tLoss_D: 0.4273\tLoss_G: 5.2753\tD(x): 0.9453\tD(G(z)): 0.2891 / 0.0076\n",
            "[84/100][0/29]\tLoss_D: 0.2224\tLoss_G: 3.9649\tD(x): 0.8568\tD(G(z)): 0.0528 / 0.0296\n",
            "[85/100][0/29]\tLoss_D: 0.5349\tLoss_G: 4.0593\tD(x): 0.8682\tD(G(z)): 0.2860 / 0.0263\n",
            "[86/100][0/29]\tLoss_D: 0.2383\tLoss_G: 3.5485\tD(x): 0.8989\tD(G(z)): 0.1123 / 0.0415\n",
            "[87/100][0/29]\tLoss_D: 0.2595\tLoss_G: 3.5688\tD(x): 0.8823\tD(G(z)): 0.1161 / 0.0390\n",
            "[88/100][0/29]\tLoss_D: 0.5308\tLoss_G: 4.3139\tD(x): 0.9174\tD(G(z)): 0.3225 / 0.0209\n",
            "[89/100][0/29]\tLoss_D: 0.2764\tLoss_G: 2.9664\tD(x): 0.8618\tD(G(z)): 0.1034 / 0.0700\n",
            "[90/100][0/29]\tLoss_D: 0.2881\tLoss_G: 3.0666\tD(x): 0.9069\tD(G(z)): 0.1555 / 0.0660\n",
            "[91/100][0/29]\tLoss_D: 0.3087\tLoss_G: 3.9524\tD(x): 0.9222\tD(G(z)): 0.1909 / 0.0267\n",
            "[92/100][0/29]\tLoss_D: 2.6793\tLoss_G: 8.4254\tD(x): 0.9977\tD(G(z)): 0.8795 / 0.0007\n",
            "[93/100][0/29]\tLoss_D: 0.4355\tLoss_G: 2.8308\tD(x): 0.7736\tD(G(z)): 0.1379 / 0.0759\n",
            "[94/100][0/29]\tLoss_D: 0.2536\tLoss_G: 3.4328\tD(x): 0.9133\tD(G(z)): 0.1406 / 0.0449\n",
            "[95/100][0/29]\tLoss_D: 0.2896\tLoss_G: 2.2014\tD(x): 0.8162\tD(G(z)): 0.0697 / 0.1520\n",
            "[96/100][0/29]\tLoss_D: 0.4465\tLoss_G: 3.0254\tD(x): 0.8150\tD(G(z)): 0.1747 / 0.0769\n",
            "[97/100][0/29]\tLoss_D: 0.2995\tLoss_G: 3.1427\tD(x): 0.8350\tD(G(z)): 0.0964 / 0.0575\n",
            "[98/100][0/29]\tLoss_D: 0.2441\tLoss_G: 3.0439\tD(x): 0.9272\tD(G(z)): 0.1437 / 0.0659\n",
            "[99/100][0/29]\tLoss_D: 0.3942\tLoss_G: 4.5255\tD(x): 0.7351\tD(G(z)): 0.0321 / 0.0213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzw4u67aoAvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOjLWsrdoHXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}